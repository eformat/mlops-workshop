{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing Using Apache Spark on Openshift\n",
    "\n",
    "This notebook server is hosted on the OpenShift platform which provides a dedicated notebook server for each individual user. The platform takes care of provisioning the cluster resources including the allocation related to storage resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually set variables in notebook mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['S3_ENDPOINT_URL'] = \"http://minio-ml-workshop:9000\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Prepare S3 URL\n",
    "Define a function that will convert an S3 URL into URL that works with MINIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, socket\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# Get the S3 URL information and use it in Spark Context\n",
    "# NOTE: S3 Hadoop API for spark does not work with domain name, use IP address instead\n",
    "def domain_to_ip(url):\n",
    "    domain = urlparse(url).netloc.split(\":\")[0]\n",
    "    ip_address = socket.gethostbyname(domain)\n",
    "    ip_url = url.replace(domain, ip_address)\n",
    "    return ip_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Connect to Spark Cluster provided by OpenShift Platform\n",
    "Using the given spark_util library, create a Spark session that connects to a Spark cluster dedicated for this notebook. You may add additional Spark submit arguments in the second argument of spark_util.getOrCreateSparkSession() such as additional packages and or override some configuration items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing environment variables for Spark\n",
      "Creating a spark session...\n",
      "Spark session created\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <br/>\n",
       "            <p><b>Spark Context</b></p>\n",
       "            <dl>\n",
       "              <dt>Cluster Name</dt>\n",
       "                <dd><code>spark-cluster-rbrigoli</code></dd>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-cluster-rbrigoli:7077</code></dd>\n",
       "              <dt>App Id</dt>\n",
       "                <dd><code>app-20211018095348-0015</code></dd>\n",
       "              <dt>App Name</dt>\n",
       "                <dd><code>ANZ Join Tables</code></dd>\n",
       "              <dt>Driver IP</dt>\n",
       "                <dd><code>10.130.3.188</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spark_util\n",
    "\n",
    "submit_args = f\"--conf spark.hadoop.fs.s3a.endpoint={domain_to_ip(os.environ['S3_ENDPOINT_URL'])} \\\n",
    "--conf spark.hadoop.fs.s3a.access.key=minio \\\n",
    "--conf spark.hadoop.fs.s3a.secret.key=minio123 \\\n",
    "--conf spark.hadoop.fs.s3a.path.style.access=true \\\n",
    "--conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem \\\n",
    "--conf spark.hadoop.fs.s3a.multipart.size=104857600 \\\n",
    "--conf spark.jars.ivy=/tmp \\\n",
    "--packages org.apache.hadoop:hadoop-aws:3.2.0\"\n",
    "\n",
    "spark = spark_util.getOrCreateSparkSession(\"ANZ Join Tables\", submit_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Create dataframes from CSV files\n",
    "\n",
    "Using Spark, read the CSV filed from S3 storage and load them as Spark dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CNCRN_ID: string (nullable = true)\n",
      " |-- SOURCE_SYSTEM: string (nullable = true)\n",
      " |-- RECVD_D: string (nullable = true)\n",
      " |-- CNCRN_DS: string (nullable = true)\n",
      " |-- STAT: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- CNCRN_ID: string (nullable = true)\n",
      " |-- ISSUE_DS: string (nullable = true)\n",
      " |-- END_DATE: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- CASENUMBER_C: string (nullable = true)\n",
      " |-- ISSUE_DS: string (nullable = true)\n",
      " |-- END_DATE: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_cfms_cncrn = spark.read\\\n",
    "                .options(delimeter=',', inferSchema='True', header='True') \\\n",
    "                .csv(\"s3a://raw-data-anz/CFMS_CNCRN.csv\")\n",
    "df_cfms_cncrn.printSchema()\n",
    "\n",
    "df_cfms_issue = spark.read\\\n",
    "                .options(delimeter=',', inferSchema='True', header='True') \\\n",
    "                .csv(\"s3a://raw-data-anz/CFMS_ISSUE.csv\")\n",
    "df_cfms_issue.printSchema()\n",
    "\n",
    "df_salesforce = spark.read\\\n",
    "                .options(delimeter=',', inferSchema='True', header='True') \\\n",
    "                .csv(\"s3a://raw-data-anz/SALESFORCECMOSAU_CASE.csv\")\n",
    "df_salesforce.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join dataframes\n",
    "Perform a full outer join on two dataframes using ```CNCRN_ID``` as key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+----------+--------+-------+-----+--------------+-----------+-----+------------------+-----------+\n",
      "|CNCRN_ID|SOURCE_SYSTEM|   RECVD_D|CNCRN_DS|   STAT|CF_ID|   CF_ISSUE_DS|CF_END_DATE|SF_ID|       SF_ISSUE_DS|SF_END_DATE|\n",
      "+--------+-------------+----------+--------+-------+-----+--------------+-----------+-----+------------------+-----------+\n",
      "|    A111|         CFMS|2019-06-14|       A|   Open|   11|K CRFMS - A111| 2050-01-01| null|              null|       null|\n",
      "|    A111|         CFMS|2019-06-14|       A|   Open|    1|A CRFMS - A111| 2025-01-01| null|              null|       null|\n",
      "|    A112|         CFMS|2020-06-12|       B|   Open|   12|L CRFMS - A112| 2050-01-01| null|              null|       null|\n",
      "|    A112|         CFMS|2020-06-12|       B|   Open|    2|B CRFMS - A112| 2025-01-01| null|              null|       null|\n",
      "|    A113|         CFMS|2021-06-01|       C|   Open|   13|M CRFMS - A113| 2050-01-01| null|              null|       null|\n",
      "|    A113|         CFMS|2021-06-01|       C|   Open|    3|C CRFMS - A113| 2025-01-01| null|              null|       null|\n",
      "|    A114|         CFMS|2019-07-23|       D|   Open|    4|D CRFMS - A114| 2025-01-01| null|              null|       null|\n",
      "|    A115|         CFMS|2019-08-24|       E| Closed|    5|E CRFMS - A115| 2025-01-01| null|              null|       null|\n",
      "|    A116|   SALESFORCE|2019-09-11|       F| Closed| null|          null|       null|    1|A SALESFORCE - 116| 2029-05-01|\n",
      "|    A117|   SALESFORCE|2019-10-12|       G|Pending| null|          null|       null|    2|B SALESFORCE - 117| 2029-05-01|\n",
      "|    A118|   SALESFORCE|2020-06-13|       H|Pending| null|          null|       null|    3|C SALESFORCE - 118| 2029-05-01|\n",
      "|    A119|   SALESFORCE|2020-11-14|       I|   Open| null|          null|       null|    4|D SALESFORCE - 119| 2029-05-01|\n",
      "|    A120|   SALESFORCE|2020-11-14|       J|   Open| null|          null|       null|    5|E SALESFORCE - 120| 2029-05-10|\n",
      "|    A121|         CFMS|2019-06-14|       K|   Open|    6|F CRFMS - A121| 2026-01-01| null|              null|       null|\n",
      "|    A122|         CFMS|2020-06-12|       L|   Open|    7|G CRFMS - A122| 2026-01-01| null|              null|       null|\n",
      "|    A123|         CFMS|2021-06-01|       M|   Open|    8|H CRFMS - A123| 2026-01-01| null|              null|       null|\n",
      "|    A124|         CFMS|2019-07-23|       N|   Open|    9|I CRFMS - A124| 2026-01-01| null|              null|       null|\n",
      "|    A125|         CFMS|2019-08-24|       O| Closed|   10|J CRFMS - A125|2026-01-210| null|              null|       null|\n",
      "|    A126|   SALESFORCE|2019-09-11|       P| Closed| null|          null|       null|    6|F SALESFORCE - 126| 2029-08-01|\n",
      "|    A127|   SALESFORCE|2019-10-12|       Q|Pending| null|          null|       null|    7|G SALESFORCE - 127| 2029-08-01|\n",
      "+--------+-------------+----------+--------+-------+-----+--------------+-----------+-----+------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_cfms_issue_alias = df_cfms_issue.select(col(\"CNCRN_ID\"), col(\"ID\").alias(\"CF_ID\"), \\\n",
    "                                           col(\"ISSUE_DS\").alias(\"CF_ISSUE_DS\"), col(\"END_DATE\").alias(\"CF_END_DATE\"))\n",
    "\n",
    "df_cfms_joined = df_cfms_cncrn.join(df_cfms_issue_alias, \"CNCRN_ID\", how=\"left_outer\")\n",
    "#df_cfms_joined.printSchema()\n",
    "\n",
    "df_salesforce_alias = df_salesforce.select(col(\"CASENUMBER_C\").alias(\"CNCRN_ID\"), col(\"ID\").alias(\"SF_ID\"), \\\n",
    "                                           col(\"ISSUE_DS\").alias(\"SF_ISSUE_DS\"), col(\"END_DATE\").alias(\"SF_END_DATE\"))\n",
    "\n",
    "df_cfms_salesforce_joined = df_cfms_joined.join(df_salesforce_alias, \"CNCRN_ID\", how=\"left_outer\")\n",
    "#df_cfms_salesforce_joined.printSchema()\n",
    "\n",
    "\n",
    "df_cfms_salesforce_joined.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Push the prepared data to the object storage and stop the Spark application\n",
    "Write the joined dataframe to an S3 bucket as CSV and as Parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "file_location = \"s3a://data-anz/cfms_concern\"\n",
    "df_cfms_salesforce_joined.write.mode(\"overwrite\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .format(\"csv\").save(file_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_location = \"s3a://data-anz/cfms_concern_parquet\"\n",
    "df_cfms_salesforce_joined.write.mode(\"overwrite\").parquet(file_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Stored Parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+----------+-------------------+-------+------------------+-----------+\n",
      "|CNCRN_ID|    Source|Date Recvd|Concern Description| Status| Issue Description|   End Date|\n",
      "+--------+----------+----------+-------------------+-------+------------------+-----------+\n",
      "|    A111|      CFMS|2019-06-14|                  A|   Open|    K CRFMS - A111| 2050-01-01|\n",
      "|    A111|      CFMS|2019-06-14|                  A|   Open|    A CRFMS - A111| 2025-01-01|\n",
      "|    A112|      CFMS|2020-06-12|                  B|   Open|    L CRFMS - A112| 2050-01-01|\n",
      "|    A112|      CFMS|2020-06-12|                  B|   Open|    B CRFMS - A112| 2025-01-01|\n",
      "|    A113|      CFMS|2021-06-01|                  C|   Open|    M CRFMS - A113| 2050-01-01|\n",
      "|    A113|      CFMS|2021-06-01|                  C|   Open|    C CRFMS - A113| 2025-01-01|\n",
      "|    A114|      CFMS|2019-07-23|                  D|   Open|    D CRFMS - A114| 2025-01-01|\n",
      "|    A115|      CFMS|2019-08-24|                  E| Closed|    E CRFMS - A115| 2025-01-01|\n",
      "|    A116|SALESFORCE|2019-09-11|                  F| Closed|A SALESFORCE - 116| 2029-05-01|\n",
      "|    A117|SALESFORCE|2019-10-12|                  G|Pending|B SALESFORCE - 117| 2029-05-01|\n",
      "|    A118|SALESFORCE|2020-06-13|                  H|Pending|C SALESFORCE - 118| 2029-05-01|\n",
      "|    A119|SALESFORCE|2020-11-14|                  I|   Open|D SALESFORCE - 119| 2029-05-01|\n",
      "|    A120|SALESFORCE|2020-11-14|                  J|   Open|E SALESFORCE - 120| 2029-05-10|\n",
      "|    A121|      CFMS|2019-06-14|                  K|   Open|    F CRFMS - A121| 2026-01-01|\n",
      "|    A122|      CFMS|2020-06-12|                  L|   Open|    G CRFMS - A122| 2026-01-01|\n",
      "|    A123|      CFMS|2021-06-01|                  M|   Open|    H CRFMS - A123| 2026-01-01|\n",
      "|    A124|      CFMS|2019-07-23|                  N|   Open|    I CRFMS - A124| 2026-01-01|\n",
      "|    A125|      CFMS|2019-08-24|                  O| Closed|    J CRFMS - A125|2026-01-210|\n",
      "|    A126|SALESFORCE|2019-09-11|                  P| Closed|F SALESFORCE - 126| 2029-08-01|\n",
      "|    A127|SALESFORCE|2019-10-12|                  Q|Pending|G SALESFORCE - 127| 2029-08-01|\n",
      "+--------+----------+----------+-------------------+-------+------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *  \n",
    "\n",
    "file_location = \"s3a://data-anz/cfms_concern_parquet\"\n",
    "df_parquet = spark.read.parquet(file_location)\n",
    "\n",
    "#df_queried.show()\n",
    "\n",
    "df_formatted = df_parquet.select(col(\"CNCRN_ID\"), col(\"SOURCE_SYSTEM\").alias(\"Source\"),\\\n",
    "                                col(\"RECVD_D\").alias(\"Date Recvd\"), col(\"CNCRN_DS\").alias(\"Concern Description\"),\\\n",
    "                                col(\"STAT\").alias(\"Status\"), concat_ws(\"\", df_parquet.CF_ISSUE_DS, df_parquet.SF_ISSUE_DS).alias(\"Issue Description\"),\\\n",
    "                                concat_ws(\"\", df_parquet.CF_END_DATE, df_parquet.SF_END_DATE).alias(\"End Date\"))\n",
    "\n",
    "df_formatted.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Stored Parquer file using SQL syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+----------+--------+-------+------------------+-----------+\n",
      "|CNCRN_ID|    SOURCE|     RECVD|CNCRN_DS|   STAT|          ISSUE_DS|   END_DATE|\n",
      "+--------+----------+----------+--------+-------+------------------+-----------+\n",
      "|    A111|      CFMS|2019-06-14|       A|   Open|    K CRFMS - A111| 2050-01-01|\n",
      "|    A111|      CFMS|2019-06-14|       A|   Open|    A CRFMS - A111| 2025-01-01|\n",
      "|    A112|      CFMS|2020-06-12|       B|   Open|    L CRFMS - A112| 2050-01-01|\n",
      "|    A112|      CFMS|2020-06-12|       B|   Open|    B CRFMS - A112| 2025-01-01|\n",
      "|    A113|      CFMS|2021-06-01|       C|   Open|    M CRFMS - A113| 2050-01-01|\n",
      "|    A113|      CFMS|2021-06-01|       C|   Open|    C CRFMS - A113| 2025-01-01|\n",
      "|    A114|      CFMS|2019-07-23|       D|   Open|    D CRFMS - A114| 2025-01-01|\n",
      "|    A115|      CFMS|2019-08-24|       E| Closed|    E CRFMS - A115| 2025-01-01|\n",
      "|    A116|SALESFORCE|2019-09-11|       F| Closed|A SALESFORCE - 116| 2029-05-01|\n",
      "|    A117|SALESFORCE|2019-10-12|       G|Pending|B SALESFORCE - 117| 2029-05-01|\n",
      "|    A118|SALESFORCE|2020-06-13|       H|Pending|C SALESFORCE - 118| 2029-05-01|\n",
      "|    A119|SALESFORCE|2020-11-14|       I|   Open|D SALESFORCE - 119| 2029-05-01|\n",
      "|    A120|SALESFORCE|2020-11-14|       J|   Open|E SALESFORCE - 120| 2029-05-10|\n",
      "|    A121|      CFMS|2019-06-14|       K|   Open|    F CRFMS - A121| 2026-01-01|\n",
      "|    A122|      CFMS|2020-06-12|       L|   Open|    G CRFMS - A122| 2026-01-01|\n",
      "|    A123|      CFMS|2021-06-01|       M|   Open|    H CRFMS - A123| 2026-01-01|\n",
      "|    A124|      CFMS|2019-07-23|       N|   Open|    I CRFMS - A124| 2026-01-01|\n",
      "|    A125|      CFMS|2019-08-24|       O| Closed|    J CRFMS - A125|2026-01-210|\n",
      "|    A126|SALESFORCE|2019-09-11|       P| Closed|F SALESFORCE - 126| 2029-08-01|\n",
      "|    A127|SALESFORCE|2019-10-12|       Q|Pending|G SALESFORCE - 127| 2029-08-01|\n",
      "+--------+----------+----------+--------+-------+------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_parquet.createOrReplaceTempView(\"concerns\")\n",
    "sqldf = spark.sql(\"SELECT CNCRN_ID, SOURCE_SYSTEM as SOURCE, RECVD_D as RECVD, CNCRN_DS, STAT,\\\n",
    "concat_ws('',CF_ISSUE_DS, SF_ISSUE_DS) as ISSUE_DS, concat_ws('',CF_END_DATE,SF_END_DATE) as END_DATE FROM concerns\")\n",
    "sqldf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pivot dataframe on ISSUE_ID\n",
    "This will create column for each value of the ISSUE_ID Column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+----------+--------+-------+--------+------------------+-----------+\n",
      "|CNCRN_ID|SOURCE_SYSTEM|RECVD_D   |CNCRN_DS|STAT   |ISSUE_ID|ISSUE_DS          |END_DATE   |\n",
      "+--------+-------------+----------+--------+-------+--------+------------------+-----------+\n",
      "|A111    |CFMS         |2019-06-14|A       |Open   |11      |K CRFMS - A111    |2050-01-01 |\n",
      "|A111    |CFMS         |2019-06-14|A       |Open   |1       |A CRFMS - A111    |2025-01-01 |\n",
      "|A112    |CFMS         |2020-06-12|B       |Open   |12      |L CRFMS - A112    |2050-01-01 |\n",
      "|A112    |CFMS         |2020-06-12|B       |Open   |2       |B CRFMS - A112    |2025-01-01 |\n",
      "|A113    |CFMS         |2021-06-01|C       |Open   |13      |M CRFMS - A113    |2050-01-01 |\n",
      "|A113    |CFMS         |2021-06-01|C       |Open   |3       |C CRFMS - A113    |2025-01-01 |\n",
      "|A114    |CFMS         |2019-07-23|D       |Open   |4       |D CRFMS - A114    |2025-01-01 |\n",
      "|A115    |CFMS         |2019-08-24|E       |Closed |5       |E CRFMS - A115    |2025-01-01 |\n",
      "|A116    |SALESFORCE   |2019-09-11|F       |Closed |1       |A SALESFORCE - 116|2029-05-01 |\n",
      "|A117    |SALESFORCE   |2019-10-12|G       |Pending|2       |B SALESFORCE - 117|2029-05-01 |\n",
      "|A118    |SALESFORCE   |2020-06-13|H       |Pending|3       |C SALESFORCE - 118|2029-05-01 |\n",
      "|A119    |SALESFORCE   |2020-11-14|I       |Open   |4       |D SALESFORCE - 119|2029-05-01 |\n",
      "|A120    |SALESFORCE   |2020-11-14|J       |Open   |5       |E SALESFORCE - 120|2029-05-10 |\n",
      "|A121    |CFMS         |2019-06-14|K       |Open   |6       |F CRFMS - A121    |2026-01-01 |\n",
      "|A122    |CFMS         |2020-06-12|L       |Open   |7       |G CRFMS - A122    |2026-01-01 |\n",
      "|A123    |CFMS         |2021-06-01|M       |Open   |8       |H CRFMS - A123    |2026-01-01 |\n",
      "|A124    |CFMS         |2019-07-23|N       |Open   |9       |I CRFMS - A124    |2026-01-01 |\n",
      "|A125    |CFMS         |2019-08-24|O       |Closed |10      |J CRFMS - A125    |2026-01-210|\n",
      "|A126    |SALESFORCE   |2019-09-11|P       |Closed |6       |F SALESFORCE - 126|2029-08-01 |\n",
      "|A127    |SALESFORCE   |2019-10-12|Q       |Pending|7       |G SALESFORCE - 127|2029-08-01 |\n",
      "+--------+-------------+----------+--------+-------+--------+------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------+-------------+----------+--------+-------+-----+\n",
      "|CNCRN_ID|SOURCE_SYSTEM|RECVD_D   |CNCRN_DS|STAT   |count|\n",
      "+--------+-------------+----------+--------+-------+-----+\n",
      "|A124    |CFMS         |2019-07-23|N       |Open   |1    |\n",
      "|A130    |SALESFORCE   |2020-11-14|T       |Open   |3    |\n",
      "|A122    |CFMS         |2020-06-12|L       |Open   |1    |\n",
      "|A126    |SALESFORCE   |2019-09-11|P       |Closed |1    |\n",
      "|A111    |CFMS         |2019-06-14|A       |Open   |2    |\n",
      "|A114    |CFMS         |2019-07-23|D       |Open   |1    |\n",
      "|A121    |CFMS         |2019-06-14|K       |Open   |1    |\n",
      "|A129    |SALESFORCE   |2020-11-14|S       |Open   |3    |\n",
      "|A127    |SALESFORCE   |2019-10-12|Q       |Pending|1    |\n",
      "|A112    |CFMS         |2020-06-12|B       |Open   |2    |\n",
      "|A123    |CFMS         |2021-06-01|M       |Open   |1    |\n",
      "|A113    |CFMS         |2021-06-01|C       |Open   |2    |\n",
      "|A117    |SALESFORCE   |2019-10-12|G       |Pending|1    |\n",
      "|A125    |CFMS         |2019-08-24|O       |Closed |1    |\n",
      "|A120    |SALESFORCE   |2020-11-14|J       |Open   |1    |\n",
      "|A116    |SALESFORCE   |2019-09-11|F       |Closed |1    |\n",
      "|A128    |SALESFORCE   |2020-06-13|R       |Pending|3    |\n",
      "|A118    |SALESFORCE   |2020-06-13|H       |Pending|1    |\n",
      "|A119    |SALESFORCE   |2020-11-14|I       |Open   |1    |\n",
      "|A115    |CFMS         |2019-08-24|E       |Closed |1    |\n",
      "+--------+-------------+----------+--------+-------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(CNCRN_ID='A124', SOURCE_SYSTEM='CFMS', RECVD_D='2019-07-23', CNCRN_DS='N', STAT='Open', 1_ISSUE_DS=None, 1_END_DATE=None, 10_ISSUE_DS=None, 10_END_DATE=None, 11_ISSUE_DS=None, 11_END_DATE=None, 12_ISSUE_DS=None, 12_END_DATE=None, 13_ISSUE_DS=None, 13_END_DATE=None, 14_ISSUE_DS=None, 14_END_DATE=None, 15_ISSUE_DS=None, 15_END_DATE=None, 16_ISSUE_DS=None, 16_END_DATE=None, 2_ISSUE_DS=None, 2_END_DATE=None, 3_ISSUE_DS=None, 3_END_DATE=None, 4_ISSUE_DS=None, 4_END_DATE=None, 5_ISSUE_DS=None, 5_END_DATE=None, 6_ISSUE_DS=None, 6_END_DATE=None, 7_ISSUE_DS=None, 7_END_DATE=None, 8_ISSUE_DS=None, 8_END_DATE=None, 9_ISSUE_DS='I CRFMS - A124', 9_END_DATE='2026-01-01')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_col_merged = df_parquet.select(col(\"CNCRN_ID\"), col(\"SOURCE_SYSTEM\"), col(\"RECVD_D\"), col(\"CNCRN_DS\"), col(\"STAT\"),\\\n",
    "                                  concat_ws(\"F\",df_parquet.CF_ID, df_parquet.SF_ID).alias(\"ISSUE_ID\"),\\\n",
    "                                  concat_ws(\"\", df_parquet.CF_ISSUE_DS, df_parquet.SF_ISSUE_DS).alias(\"ISSUE_DS\"),\\\n",
    "                                  concat_ws(\"\", df_parquet.CF_END_DATE, df_parquet.SF_END_DATE).alias(\"END_DATE\"))\n",
    "\n",
    "df_col_merged.show(20, False)\n",
    "\n",
    "df_grouped = df_col_merged.groupBy(\"CNCRN_ID\", \"SOURCE_SYSTEM\", \"RECVD_D\",\"CNCRN_DS\",\"STAT\").count()\n",
    "df_grouped.show(20, False)\n",
    "\n",
    "df_pivoted = df_col_merged.groupBy(\"CNCRN_ID\", \"SOURCE_SYSTEM\", \"RECVD_D\",\"CNCRN_DS\",\"STAT\")\\\n",
    ".pivot(\"ISSUE_ID\").agg(first(\"ISSUE_DS\").alias(\"ISSUE_DS\"), first(\"END_DATE\").alias(\"END_DATE\"))\n",
    "\n",
    "file_location = \"s3a://data-anz/cfms_concern_csv_pivot\"\n",
    "df_pivoted.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\")\\\n",
    "    .format(\"csv\").save(file_location)\n",
    "\n",
    "df_pivoted.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate column values of multiple rows into a single column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+----------+--------+-------+------------------------------------------------------------+---------------------------------------+\n",
      "|CNCRN_ID|SOURCE_SYSTEM|RECVD_D   |CNCRN_DS|STAT   |ISSUE_DS                                                    |END_DATES                              |\n",
      "+--------+-------------+----------+--------+-------+------------------------------------------------------------+---------------------------------------+\n",
      "|A120    |SALESFORCE   |2020-11-14|J       |Open   |[E SALESFORCE - 120]                                        |[2029-05-10]                           |\n",
      "|A121    |CFMS         |2019-06-14|K       |Open   |[F CRFMS - A121]                                            |[2026-01-01]                           |\n",
      "|A116    |SALESFORCE   |2019-09-11|F       |Closed |[A SALESFORCE - 116]                                        |[2029-05-01]                           |\n",
      "|A129    |SALESFORCE   |2020-11-14|S       |Open   |[O SALESFORCE - 129, L SALESFORCE - 129, I SALESFORCE - 129]|[2029-08-01, 2029-08-01, 2029-08-01]   |\n",
      "|A125    |CFMS         |2019-08-24|O       |Closed |[J CRFMS - A125]                                            |[2026-01-210]                          |\n",
      "|A119    |SALESFORCE   |2020-11-14|I       |Open   |[D SALESFORCE - 119]                                        |[2029-05-01]                           |\n",
      "|A127    |SALESFORCE   |2019-10-12|Q       |Pending|[G SALESFORCE - 127]                                        |[2029-08-01]                           |\n",
      "|A118    |SALESFORCE   |2020-06-13|H       |Pending|[C SALESFORCE - 118]                                        |[2029-05-01]                           |\n",
      "|A126    |SALESFORCE   |2019-09-11|P       |Closed |[F SALESFORCE - 126]                                        |[2029-08-01]                           |\n",
      "|A123    |CFMS         |2021-06-01|M       |Open   |[H CRFMS - A123]                                            |[2026-01-01]                           |\n",
      "|A124    |CFMS         |2019-07-23|N       |Open   |[I CRFMS - A124]                                            |[2026-01-01]                           |\n",
      "|A128    |SALESFORCE   |2020-06-13|R       |Pending|[N SALESFORCE - 128, K SALESFORCE - 128, H SALESFORCE - 128]|[2029-08-01, 2029-08-01, 2029-08-01]   |\n",
      "|A114    |CFMS         |2019-07-23|D       |Open   |[D CRFMS - A114]                                            |[2025-01-01]                           |\n",
      "|A122    |CFMS         |2020-06-12|L       |Open   |[G CRFMS - A122]                                            |[2026-01-01]                           |\n",
      "|A115    |CFMS         |2019-08-24|E       |Closed |[E CRFMS - A115]                                            |[2025-01-01]                           |\n",
      "|A113    |CFMS         |2021-06-01|C       |Open   |[M CRFMS - A113, C CRFMS - A113]                            |[2050-01-01, 2025-01-01]               |\n",
      "|A117    |SALESFORCE   |2019-10-12|G       |Pending|[B SALESFORCE - 117]                                        |[2029-05-01]                           |\n",
      "|A130    |SALESFORCE   |2020-11-14|T       |Open   |[P SALESFORCE - 120, M SALESFORCE - 120, J SALESFORCE - 120]|[2029-08-210, 2029-08-210, 2029-08-210]|\n",
      "|A111    |CFMS         |2019-06-14|A       |Open   |[K CRFMS - A111, A CRFMS - A111]                            |[2050-01-01, 2025-01-01]               |\n",
      "|A112    |CFMS         |2020-06-12|B       |Open   |[L CRFMS - A112, B CRFMS - A112]                            |[2050-01-01, 2025-01-01]               |\n",
      "+--------+-------------+----------+--------+-------+------------------------------------------------------------+---------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_parent = df_col_merged.select(\"CNCRN_ID\", \"SOURCE_SYSTEM\", \"RECVD_D\",\"CNCRN_DS\",\"STAT\").distinct()\n",
    "df_sublist = df_col_merged.groupBy(\"CNCRN_ID\").agg(collect_list(\"ISSUE_DS\").alias(\"ISSUE_DS\"), collect_list(\"END_DATE\").alias(\"END_DATES\"))\n",
    "\n",
    "df_parent.join(df_sublist, \"CNCRN_ID\", how=\"inner\").show(20, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Spark Session\n",
    "Because this is last step of our data preparation, we don't need the Spark cluster anymore. We will stop the Spark context which will remove the Spark application from the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "545e036c4b32438aced1f6b3c8d38ca151d9c36189e05839cb0aa568fda70ddd"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
