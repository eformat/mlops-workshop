{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing Using Apache Spark on Openshift\n",
    "\n",
    "This notebook server is hosted on the OpenShift platform which provides a dedicated notebook server for each individual user. The platform takes care of provisioning the cluster resources including the allocation related to storage resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install and import required libraries, watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# %pip install watermark\n",
    "# %pip install Minio\n",
    "# %pip install pyspark\n",
    "# %pip install matplotlib\n",
    "\n",
    "# import json\n",
    "# import watermark\n",
    "# from minio import Minio\n",
    "\n",
    "# %matplotlib inline\n",
    "# %load_ext watermark\n",
    "# %watermark -n -v -m -g -iv\n",
    "# %pip install pyspark\n",
    "\n",
    "#os.environ['S3_ENDPOINT'] = \"http://minio-ml-workshop:9000\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Connect to Spark Cluster provided by OpenShift Platform\n",
    "Using the given spark_util library, create a Spark session that connects to a Spark cluster dedicated for this notebook. You may add additional Spark submit arguments in the second argument of spark_util.getOrCreateSparkSession() such as additional packages and or override some configuration items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import spark_util\n",
    "\n",
    "s3_config = f\"--conf spark.hadoop.fs.s3a.endpoint={os.environ['S3_ENDPOINT']} \\\n",
    "--conf spark.hadoop.fs.s3a.access.key=minio \\\n",
    "--conf spark.hadoop.fs.s3a.secret.key=minio123 \\\n",
    "--conf spark.hadoop.fs.s3a.path.style.access=true \\\n",
    "--conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem\"\n",
    "\n",
    "spark = spark_util.getOrCreateSparkSession(\"CustomerChurn\", s3_config, \"INFO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Create dataframes from CSV files\n",
    "\n",
    "Using Spark, read the CSV filed from S3 storage and load them as Spark dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataFrame_Customer = spark.read\\\n",
    "                .options(delimeter=',', inferSchema='True', header='True') \\\n",
    "                .csv(\"s3a://rawdata/Customer-Churn_P1.csv\")\n",
    "dataFrame_Customer.printSchema()\n",
    "\n",
    "dataFrame_Products = spark.read\\\n",
    "                .options(delimeter=',', inferSchema='True', header='True') \\\n",
    "                .csv(\"s3a://rawdata/Customer-Churn_P2.csv\")\n",
    "dataFrame_Products.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from Kafka\n",
    "You may also read data from a KAfka topic and create a Spark dataframe out of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.types import *\n",
    "# from  pyspark.sql.functions import *\n",
    "\n",
    "# srcKafkaBrokers = \"odh-message-bus-kafka-bootstrap:9092\"\n",
    "# srcKakaTopic = \"datatelco\"\n",
    "\n",
    "\n",
    "\n",
    "# schema = StructType()\\\n",
    "#     .add(\"customerID\", IntegerType())\\\n",
    "#     .add(\"PhoneService\", StringType())\\\n",
    "#     .add(\"MultipleLines\", StringType())\\\n",
    "#     .add(\"InternetService\", StringType())\\\n",
    "#     .add(\"OnlineSecurity\", StringType())\\\n",
    "#     .add(\"OnlineBackup\", StringType())\\\n",
    "#     .add(\"DeviceProtection\", StringType())\\\n",
    "#     .add(\"TechSupport\", StringType())\\\n",
    "#     .add(\"StreamingTV\", StringType())\\\n",
    "#     .add(\"StreamingMovies\", StringType())\\\n",
    "#     .add(\"Contract\", StringType())\\\n",
    "#     .add(\"PaperlessBilling\", StringType())\\\n",
    "#     .add(\"PaymentMethod\", StringType())\\\n",
    "#     .add(\"MonthlyCharges\", StringType())\\\n",
    "#     .add(\"TotalCharges\", DoubleType())\\\n",
    "#     .add(\"Churn\", StringType())\n",
    "\n",
    "\n",
    "\n",
    "# #Read from JSON Kafka messages into a dataframe\n",
    "# dfKafka = spark.read.format(\"kafka\")\\\n",
    "#     .option(\"kafka.bootstrap.servers\", srcKafkaBrokers)\\\n",
    "#     .option(\"subscribe\", srcKakaTopic)\\\n",
    "#     .option(\"startingOffsets\", \"earliest\")\\\n",
    "#     .load()\\\n",
    "#     .withColumn(\"value\", regexp_replace(col(\"value\").cast(\"string\"), \"\\\\\\\\\", \"\")) \\\n",
    "#     .withColumn(\"value\", regexp_replace(col(\"value\"), \"^\\\"|\\\"$\", \"\")) \\\n",
    "#     .selectExpr(\"CAST(value AS STRING) as jsonValue\")\\\n",
    "#     .rdd.map(lambda row: row[\"jsonValue\"])\n",
    "\n",
    "# dataFrame_Products = spark.read.schema(schema).json(dfKafka)\n",
    "# dataFrame_Products.printSchema()\n",
    "# dataFrame_Products.show(n=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join dataframes\n",
    "Perform a full outer join on two dataframes using ```customerID``` as key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "dataFrom_All = dataFrame_Customer.join(dataFrame_Products, \"customerID\", how=\"full\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Push the prepared data to the object storage and stop the Spark application\n",
    "Write the joined dataframe to an S3 bucket. Because this is last step of our data preparation, we don't need the Spark cluster anymore. We will stop the Spark context which will remove the Spark application from the cluster.\n",
    "\n",
    "<span style=\"color:red\">Note: Change this value of user_id to your assigned username (something in the range user1 ... user30)</span>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "user_id = \"user29\"\n",
    "file_location = \"s3a://data/full_data_csv\" + user_id\n",
    "dataFrom_All.repartition(1).write.mode(\"overwrite\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .format(\"csv\").save(file_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "545e036c4b32438aced1f6b3c8d38ca151d9c36189e05839cb0aa568fda70ddd"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
