{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Load the requirements"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as tk\n",
    "import tensorflow.keras.layers as tkl\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "import mlflow\n",
    "import numpy as np\n",
    "# import mlflow.keras\n",
    "import mlflow.tensorflow\n",
    "import joblib\n",
    "from tensorflow_addons.optimizers import Yogi\n",
    "# from tensorflow import keras\n",
    "# from tensorflow.keras import layers\n",
    "# from verta.utils import ModelAPI\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init Plugin\n",
      "Init Graph Optimizer\n",
      "Init Kernel\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\n",
    "\n",
    "# HOST = \"http://localhost:5000\"\n",
    "HOST = \"http://127.0.0.1:5000\"\n",
    "\n",
    "\n",
    "PROJECT_NAME = \"CustomerChurn\"\n",
    "EXPERIMENT_NAME = \"TFlowSaeed02\"\n",
    "\n",
    "os.environ['MLFLOW_S3_ENDPOINT_URL']='http://localhost:9000'\n",
    "os.environ['AWS_ACCESS_KEY_ID']='AKIAIOSFODNN7EXAMPLE'\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']='wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY'\n",
    "os.environ['AWS_REGION']='us-east-1'\n",
    "os.environ['AWS_BUCKET_NAME']='mlflow'\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Do the hyper parameter initializing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "directory = \"/Users/skasmani/Downloads/Redhat/codes/dataset/catsanddog/PetImages\"\n",
    "\n",
    "batch_size = 128\n",
    "NUM_CLASSES = 2\n",
    "IMG_SIZE = 224\n",
    "lr = 0.01"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Load the dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### First, load this data into the model using the image data off disk with tf.keras.preprocessing.image_dataset_from_directory, which will generate a tf.data.Dataset:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "## best practice is to create folders for train and test data\n",
    "\n",
    "train_data = tk.preprocessing.image_dataset_from_directory(\n",
    "        directory,\n",
    "        image_size = (IMG_SIZE, IMG_SIZE),\n",
    "        batch_size = batch_size,\n",
    "        subset = \"training\",\n",
    "        label_mode = 'categorical',\n",
    "        validation_split=0.2,\n",
    "        seed=100\n",
    "    )\n",
    "\n",
    "validation_data = tk.preprocessing.image_dataset_from_directory(\n",
    "    directory,\n",
    "    image_size = (IMG_SIZE, IMG_SIZE),\n",
    "    batch_size = batch_size,\n",
    "    subset = \"validation\",\n",
    "    label_mode = 'categorical',\n",
    "    validation_split=0.2,\n",
    "    seed=100\n",
    ")\n",
    "class_names = np.array(train_data.class_names)\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Second, because TensorFlow Hub's convention for image models is to expect float inputs in the [0, 1] range, use the tf.keras.layers.experimental.preprocessing.Rescaling layer to achieve this."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# normalization_layer = tkl.experimental.preprocessing.Rescaling(1./255)\n",
    "normalization_layer = tkl.experimental.preprocessing.Rescaling(1)\n",
    "train_data = train_data.map(lambda x, y: (normalization_layer(x), y)) # Where x—images, y—labels.\n",
    "validation_data = validation_data.map(lambda x, y: (normalization_layer(x), y)) # Where x—images, y—labels.\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Third, finish the input pipeline by using buffered prefetching with Dataset.prefetch, so you can yield the data from disk without I/O blocking issues.\n",
    "\n",
    "These are some of the most important tf.data methods you should use when loading data. Interested readers can learn more about them, as well as how to cache data to disk and other techniques, in the Better performance with the tf.data API guide."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_data = train_data.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "validation_data = validation_data.cache().prefetch(buffer_size=AUTOTUNE)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### print dataset information"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for image_batch, labels_batch in train_data:\n",
    "    print(image_batch.shape)\n",
    "    print(labels_batch.shape)\n",
    "    break\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Do initialization with mlflow"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "# # Connect to local MLflow tracking server\n",
    "# mlflow.set_tracking_uri(HOST)\n",
    "\n",
    "# # Set the experiment name...\n",
    "# mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "# # _original_keras_log_model = mlflow.keras.log_model\n",
    "\n",
    "# # def _keras_log_model_patch(*args, **kwargs):\n",
    "# #     kwargs[\"save_format\"] = \"tf\"\n",
    "# #     _original_keras_log_model(*args, **kwargs)\n",
    "\n",
    "# # mlflow.keras.log_model = _keras_log_model_patch\n",
    "\n",
    "\n",
    "\n",
    "# # mlflow.tensorflow.autolog(every_n_iter=1)\n",
    "# mlflow.keras.autolog()\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##  5. Define the model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras import layers\n",
    "\n",
    "img_augmentation = tk.Sequential(\n",
    "    [\n",
    "        tkl.experimental.preprocessing.RandomRotation(factor=0.15),\n",
    "        tkl.experimental.preprocessing.RandomTranslation(height_factor=0.1, width_factor=0.1),\n",
    "        tkl.experimental.preprocessing.RandomFlip(),\n",
    "        tkl.experimental.preprocessing.RandomContrast(factor=0.1),\n",
    "    ],\n",
    "    name=\"img_augmentation\",\n",
    ")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# !pip install tensorflow_hub"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# feature_extractor_model = \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b0/feature_vector/2\"\n",
    "# feature_extractor_layer = hub.KerasLayer(\n",
    "#     feature_extractor_model,\n",
    "#     input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "#     trainable=False)\n",
    "feature_extractor_layer = EfficientNetB0(include_top=False, weights='imagenet',input_shape=(IMG_SIZE, IMG_SIZE, 3),pooling = 'avg')\n",
    "feature_extractor_layer.trainable = False\n",
    "\n",
    "\n",
    "\n",
    "# feature_extractor_layer.trainable = True\n",
    "# for layer in feature_extractor_layer.layers[:-5]:\n",
    "#     layer.trainable =  False"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# NUM_CLASSES = len(class_names)\n",
    "\n",
    "\n",
    "# # create the base pre-trained model\n",
    "# base_model = EfficientNetB0(weights='imagenet', include_top=False)\n",
    "# base_model.trainable = False\n",
    "# # add a global spatial average pooling layer\n",
    "# x = base_model.output\n",
    "# x = tkl.GlobalAveragePooling2D()(x)\n",
    "# # let's add a fully-connected layer\n",
    "# # x = tkl.Dense(1024, activation='relu')(x)\n",
    "# # and a logistic layer -- let's say we have 200 classes\n",
    "# predictions = tkl.Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "\n",
    "# # this is the model we will train\n",
    "# model = tk.models.Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "\n",
    "\n",
    "# mode = tf.keras.models.Sequential([\n",
    "#     tf.keras.layers.InputLayer(input_shape=[224, 224, 3]),\n",
    "#     effnetv2_model.get_model('efficientnetv2-b0', include_top=False),\n",
    "#     tf.keras.layers.Dropout(rate=0.2),\n",
    "#     tf.keras.layers.Dense(4, activation='softmax'),\n",
    "# ])\n",
    "# model.summary()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# !pip install coremltools\n",
    "!pip install tensorflow-addons"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "NUM_CLASSES = len(class_names)\n",
    "\n",
    "model = tk.Sequential([\n",
    "  # tk.Input(shape=(224, 224, 3)),\n",
    "  feature_extractor_layer,\n",
    "  # tkl.GlobalAveragePooling2D(),\n",
    "  tkl.Dropout(rate = 0.2),\n",
    "  tkl.Dense(NUM_CLASSES, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Compile the model with loss and optimizer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.compile(\n",
    "        optimizer=tf.optimizers.Adam(1e-2), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    "    )\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "##  6.Train the Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "epochs = 5\n",
    "training_history = model.fit(train_data, epochs=epochs, validation_data=validation_data, verbose=2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Save the model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.save(\"ImageClassificationPredictor.h5\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class_names"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "joblib.dump(class_names, 'ImageClassificationClassNamesPredictor.pkl')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "model1 = tk.models.load_model('ImageClassificationPredictor.h5',custom_objects={'KerasLayer':hub.KerasLayer})\n",
    "from PIL import Image\n",
    "import io\n",
    "from urllib import request\n",
    "import json\n",
    "\n",
    "\n",
    "request1 = {\"data\":\n",
    "{\n",
    "\n",
    "\n",
    "\"path\":\n",
    "[\n",
    "\"https://imageprocessor.digital.vistaprint.com/crop/945,117,3210x3210/maxWidth/1000/stockservice.digital.vistaprint.com/9971027207cbc734104f4b327a4118dd.jpg\"\n",
    "],\n",
    "\"label\": [[\"Dog\"]]\n",
    "\n",
    "}\n",
    "}\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "\n",
    "print(request1.get(\"data\", {}))\n",
    "img_path = request1.get(\"data\", {}).get(\"path\")\n",
    "print(str(img_path[0]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'path': ['https://imageprocessor.digital.vistaprint.com/crop/945,117,3210x3210/maxWidth/1000/stockservice.digital.vistaprint.com/9971027207cbc734104f4b327a4118dd.jpg'], 'label': [['Dog']]}\n",
      "https://imageprocessor.digital.vistaprint.com/crop/945,117,3210x3210/maxWidth/1000/stockservice.digital.vistaprint.com/9971027207cbc734104f4b327a4118dd.jpg\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "\n",
    "res = request.urlopen(str(img_path[0])).read()\n",
    "img_array = Image.open(io.BytesIO(res)).resize((224, 224))\n",
    "# img = tf.keras.preprocessing.image.load_img(str(img_path[0]), target_size=(224, 224))\n",
    "img_array = tf.keras.preprocessing.image.img_to_array(img_array)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "img_array = tf.keras.preprocessing.image.img_to_array(img_array)\n",
    "img_array = img_array/255\n",
    "img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
    "\n",
    "# predictions = model1.predict(img_array)\n",
    "predictions = model1(img_array)\n",
    "\n",
    "predictions"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.27777043, 0.72222954]], dtype=float32)>"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "modelsss = tk.models.load_model('CatAndDog.model.h5',custom_objects={'KerasLayer':hub.KerasLayer})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model2 = tf.saved_model.load('/Users/skasmani/Downloads/Redhat/Redhat_git/ml-training/deep-learning/others/mlruns/1/2eddbdfb09af4561946d81d9b9b8c866/artifacts/model/data/model/saved_model.pb')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "# # feature_extractor_layer,\n",
    "# # tf.keras.layers.Dense(num_classes)\n",
    "\n",
    "# # m = tf.keras.Sequential([\n",
    "# #     hub.KerasLayer(\"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b1/feature_vector/2\",\n",
    "# #                    trainable=False),  # Can be True, see below.\n",
    "# #     tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "# # ])\n",
    "# # m.build([None, 240, 240, 3])  # Batch input shape.\n",
    "\n",
    "# with strategy.scope():\n",
    "#     # inputs = tkl.experimental.preprocessing.Rescaling(1./255, input_shape=(IMG_SIZE, IMG_SIZE, 3)),\n",
    "#     inputs = tkl.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "#     # inputs = tkl.experimental.preprocessing.Rescaling(1./255)(inputs)\n",
    "#     x = img_augmentation(inputs)\n",
    "#     x = feature_extractor_layer(x)\n",
    "#     outputs = tkl.Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "\n",
    "#     # outputs = EfficientNetB0(include_top=True, weights=None, classes=NUM_CLASSES)(x)\n",
    "\n",
    "#     model = tk.Model(inputs, outputs)\n",
    "\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "# epochs = 1  # @param {type: \"slider\", min:10, max:100}\n",
    "# hist = model.fit(train_data, epochs=epochs, validation_data=validation_data, verbose=2)"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.save(\"CatAndDog.model.h5\")"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "accuracy = training_history.history[\"accuracy\"]\n",
    "val_accuracy = training_history.history[\"val_accuracy\"]\n",
    "epochs=range(1,epochs)\n",
    "plt.plot(epochs,accuracy, label=\"Training Accuracy\")\n",
    "plt.plot(epochs,val_accuracy, label=\"Validation Accuracy\")\n",
    "\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.legend()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "accuracy = training_history.history[\"loss\"]\n",
    "val_accuracy = training_history.history[\"val_loss\"]\n",
    "epochs=range(1,epochs)\n",
    "plt.plot(epochs,accuracy,  label=\"Training Loss\")\n",
    "plt.plot(epochs,val_accuracy,  label=\"Validation Loss\")\n",
    "\n",
    "\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.legend()\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('AI_ML_38_GPU': conda)"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "interpreter": {
   "hash": "10d220c55dad87bc296f91a276cffc08c658df4f112171a2982baf6251a25e4e"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}