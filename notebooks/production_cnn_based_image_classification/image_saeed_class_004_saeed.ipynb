{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Load the requirements"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as tk\n",
    "import tensorflow.keras.layers as tkl\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "import mlflow\n",
    "import numpy as np\n",
    "# import mlflow.keras\n",
    "import mlflow.tensorflow\n",
    "import joblib\n",
    "from tensorflow_addons.optimizers import Yogi\n",
    "# from tensorflow import keras\n",
    "# from tensorflow.keras import layers\n",
    "# from verta.utils import ModelAPI\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import os\n",
    "\n",
    "# HOST = \"http://localhost:5000\"\n",
    "HOST = \"http://127.0.0.1:5000\"\n",
    "\n",
    "\n",
    "PROJECT_NAME = \"CustomerChurn\"\n",
    "EXPERIMENT_NAME = \"TFlowSaeed02\"\n",
    "\n",
    "os.environ['MLFLOW_S3_ENDPOINT_URL']='http://localhost:9000'\n",
    "os.environ['AWS_ACCESS_KEY_ID']='AKIAIOSFODNN7EXAMPLE'\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']='wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY'\n",
    "os.environ['AWS_REGION']='us-east-1'\n",
    "os.environ['AWS_BUCKET_NAME']='mlflow'\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Do the hyper parameter initializing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "\n",
    "directory = \"/Users/skasmani/Downloads/Redhat/codes/dataset/catsanddog/PetImages\"\n",
    "\n",
    "batch_size = 128\n",
    "NUM_CLASSES = 2\n",
    "IMG_SIZE = 224\n",
    "lr = 0.01"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Load the dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### First, load this data into the model using the image data off disk with tf.keras.preprocessing.image_dataset_from_directory, which will generate a tf.data.Dataset:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "\n",
    "## best practice is to create folders for train and test data\n",
    "\n",
    "train_data = tk.preprocessing.image_dataset_from_directory(\n",
    "        directory,\n",
    "        image_size = (IMG_SIZE, IMG_SIZE),\n",
    "        batch_size = batch_size,\n",
    "        subset = \"training\",\n",
    "        label_mode = 'categorical',\n",
    "        validation_split=0.2,\n",
    "        seed=100\n",
    "    )\n",
    "\n",
    "validation_data = tk.preprocessing.image_dataset_from_directory(\n",
    "    directory,\n",
    "    image_size = (IMG_SIZE, IMG_SIZE),\n",
    "    batch_size = batch_size,\n",
    "    subset = \"validation\",\n",
    "    label_mode = 'categorical',\n",
    "    validation_split=0.2,\n",
    "    seed=100\n",
    ")\n",
    "class_names = np.array(train_data.class_names)\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 23410 files belonging to 2 classes.\n",
      "Using 18728 files for training.\n",
      "Metal device set to: AMD Radeon Pro 555X\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 2.00 GB\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-09-29 13:42:56.584720: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-29 13:42:56.615110: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2021-09-29 13:42:56.618033: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 23410 files belonging to 2 classes.\n",
      "Using 4682 files for validation.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Second, because TensorFlow Hub's convention for image models is to expect float inputs in the [0, 1] range, use the tf.keras.layers.experimental.preprocessing.Rescaling layer to achieve this."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# normalization_layer = tkl.experimental.preprocessing.Rescaling(1./255)\n",
    "normalization_layer = tkl.experimental.preprocessing.Rescaling(1)\n",
    "train_data = train_data.map(lambda x, y: (normalization_layer(x), y)) # Where x—images, y—labels.\n",
    "validation_data = validation_data.map(lambda x, y: (normalization_layer(x), y)) # Where x—images, y—labels.\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Third, finish the input pipeline by using buffered prefetching with Dataset.prefetch, so you can yield the data from disk without I/O blocking issues.\n",
    "\n",
    "These are some of the most important tf.data methods you should use when loading data. Interested readers can learn more about them, as well as how to cache data to disk and other techniques, in the Better performance with the tf.data API guide."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_data = train_data.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "validation_data = validation_data.cache().prefetch(buffer_size=AUTOTUNE)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### print dataset information"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "for image_batch, labels_batch in train_data:\n",
    "    print(image_batch.shape)\n",
    "    print(labels_batch.shape)\n",
    "    break\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-09-29 13:43:00.468622: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Corrupt JPEG data: 162 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(128, 224, 224, 3)\n",
      "(128, 2)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-09-29 13:43:04.202128: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Do initialization with mlflow"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "\n",
    "\n",
    "# # Connect to local MLflow tracking server\n",
    "# mlflow.set_tracking_uri(HOST)\n",
    "\n",
    "# # Set the experiment name...\n",
    "# mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "# # _original_keras_log_model = mlflow.keras.log_model\n",
    "\n",
    "# # def _keras_log_model_patch(*args, **kwargs):\n",
    "# #     kwargs[\"save_format\"] = \"tf\"\n",
    "# #     _original_keras_log_model(*args, **kwargs)\n",
    "\n",
    "# # mlflow.keras.log_model = _keras_log_model_patch\n",
    "\n",
    "\n",
    "\n",
    "# # mlflow.tensorflow.autolog(every_n_iter=1)\n",
    "# mlflow.keras.autolog()\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##  5. Define the model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras import layers\n",
    "\n",
    "img_augmentation = tk.Sequential(\n",
    "    [\n",
    "        tkl.experimental.preprocessing.RandomRotation(factor=0.15),\n",
    "        tkl.experimental.preprocessing.RandomTranslation(height_factor=0.1, width_factor=0.1),\n",
    "        tkl.experimental.preprocessing.RandomFlip(),\n",
    "        tkl.experimental.preprocessing.RandomContrast(factor=0.1),\n",
    "    ],\n",
    "    name=\"img_augmentation\",\n",
    ")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# !pip install tensorflow_hub"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# feature_extractor_model = \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b0/feature_vector/2\"\n",
    "# feature_extractor_layer = hub.KerasLayer(\n",
    "#     feature_extractor_model,\n",
    "#     input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "#     trainable=False)\n",
    "# feature_extractor_layer = EfficientNetB0(include_top=False, weights='imagenet',input_shape=(IMG_SIZE, IMG_SIZE, 3),pooling = 'avg')\n",
    "# feature_extractor_layer.trainable = False\n",
    "\n",
    "\n",
    "\n",
    "# feature_extractor_layer.trainable = True\n",
    "# for layer in feature_extractor_layer.layers[:-5]:\n",
    "#     layer.trainable =  False"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "The `weights` argument should be either `None` (random initialization), `imagenet` (pre-training on ImageNet), or the path to the weights file to be loaded.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/bc/2l89x1hs7f5c0gv5q2_lj11r0000gn/T/ipykernel_4188/3776524848.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# feature_extractor_layer.trainable = False\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mfeature_extractor_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEfficientNetB0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"efficientnetb0_notop.h5\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_top\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mfeature_extractor_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/AI_ML_38_GPU/lib/python3.8/site-packages/tensorflow/python/keras/applications/efficientnet.py\u001b[0m in \u001b[0;36mEfficientNetB0\u001b[0;34m(include_top, weights, input_tensor, input_shape, pooling, classes, classifier_activation, **kwargs)\u001b[0m\n\u001b[1;32m    533\u001b[0m                    \u001b[0mclassifier_activation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m                    **kwargs):\n\u001b[0;32m--> 535\u001b[0;31m   return EfficientNet(\n\u001b[0m\u001b[1;32m    536\u001b[0m       \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m       \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/AI_ML_38_GPU/lib/python3.8/site-packages/tensorflow/python/keras/applications/efficientnet.py\u001b[0m in \u001b[0;36mEfficientNet\u001b[0;34m(width_coefficient, depth_coefficient, default_size, dropout_rate, drop_connect_rate, depth_divisor, activation, blocks_args, model_name, include_top, weights, input_tensor, input_shape, pooling, classes, classifier_activation)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'imagenet'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_exists_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m     raise ValueError('The `weights` argument should be either '\n\u001b[0m\u001b[1;32m    278\u001b[0m                      \u001b[0;34m'`None` (random initialization), `imagenet` '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m                      \u001b[0;34m'(pre-training on ImageNet), '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The `weights` argument should be either `None` (random initialization), `imagenet` (pre-training on ImageNet), or the path to the weights file to be loaded."
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "!python efficientnet_weight_update_util.py --model b0 --notop --ckpt \\\n",
    "        efficientnet-b1/model.ckpt --o efficientnetb0_notop.h5\n",
    "        \n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "feature_extractor_layer = EfficientNetB0(weights=\"efficientnetb0_notop.h5\", include_top=False)\n",
    "feature_extractor_layer.trainable = False"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "The `weights` argument should be either `None` (random initialization), `imagenet` (pre-training on ImageNet), or the path to the weights file to be loaded.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/bc/2l89x1hs7f5c0gv5q2_lj11r0000gn/T/ipykernel_4188/2797153450.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfeature_extractor_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEfficientNetB0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"efficientnetb0_notop.h5\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_top\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfeature_extractor_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/AI_ML_38_GPU/lib/python3.8/site-packages/tensorflow/python/keras/applications/efficientnet.py\u001b[0m in \u001b[0;36mEfficientNetB0\u001b[0;34m(include_top, weights, input_tensor, input_shape, pooling, classes, classifier_activation, **kwargs)\u001b[0m\n\u001b[1;32m    533\u001b[0m                    \u001b[0mclassifier_activation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m                    **kwargs):\n\u001b[0;32m--> 535\u001b[0;31m   return EfficientNet(\n\u001b[0m\u001b[1;32m    536\u001b[0m       \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m       \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/AI_ML_38_GPU/lib/python3.8/site-packages/tensorflow/python/keras/applications/efficientnet.py\u001b[0m in \u001b[0;36mEfficientNet\u001b[0;34m(width_coefficient, depth_coefficient, default_size, dropout_rate, drop_connect_rate, depth_divisor, activation, blocks_args, model_name, include_top, weights, input_tensor, input_shape, pooling, classes, classifier_activation)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'imagenet'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_exists_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m     raise ValueError('The `weights` argument should be either '\n\u001b[0m\u001b[1;32m    278\u001b[0m                      \u001b[0;34m'`None` (random initialization), `imagenet` '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m                      \u001b[0;34m'(pre-training on ImageNet), '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The `weights` argument should be either `None` (random initialization), `imagenet` (pre-training on ImageNet), or the path to the weights file to be loaded."
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# NUM_CLASSES = len(class_names)\n",
    "\n",
    "\n",
    "# # create the base pre-trained model\n",
    "# base_model = EfficientNetB0(weights='imagenet', include_top=False)\n",
    "# base_model.trainable = False\n",
    "# # add a global spatial average pooling layer\n",
    "# x = base_model.output\n",
    "# x = tkl.GlobalAveragePooling2D()(x)\n",
    "# # let's add a fully-connected layer\n",
    "# # x = tkl.Dense(1024, activation='relu')(x)\n",
    "# # and a logistic layer -- let's say we have 200 classes\n",
    "# predictions = tkl.Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "\n",
    "# # this is the model we will train\n",
    "# model = tk.models.Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "\n",
    "\n",
    "# mode = tf.keras.models.Sequential([\n",
    "#     tf.keras.layers.InputLayer(input_shape=[224, 224, 3]),\n",
    "#     effnetv2_model.get_model('efficientnetv2-b0', include_top=False),\n",
    "#     tf.keras.layers.Dropout(rate=0.2),\n",
    "#     tf.keras.layers.Dense(4, activation='softmax'),\n",
    "# ])\n",
    "# model.summary()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# !pip install coremltools\n",
    "!pip install tensorflow-addons"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "NUM_CLASSES = len(class_names)\n",
    "\n",
    "model = tk.Sequential([\n",
    "  # tk.Input(shape=(224, 224, 3)),\n",
    "  feature_extractor_layer,\n",
    "  # tkl.GlobalAveragePooling2D(),\n",
    "  tkl.Dropout(rate = 0.2),\n",
    "  tkl.Dense(NUM_CLASSES, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Compile the model with loss and optimizer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.compile(\n",
    "        optimizer=tf.optimizers.Adam(1e-2), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    "    )\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "##  6.Train the Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "epochs = 5\n",
    "training_history = model.fit(train_data, epochs=epochs, validation_data=validation_data, verbose=2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Save the model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.save(\"ImageClassificationPredictor.h5\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class_names"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "joblib.dump(class_names, 'ImageClassificationClassNamesPredictor.pkl')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "model1 = tk.models.load_model('ImageClassificationPredictor.h5',custom_objects={'KerasLayer':hub.KerasLayer})\n",
    "from PIL import Image\n",
    "import io\n",
    "from urllib import request\n",
    "import json\n",
    "\n",
    "\n",
    "request1 = {\"data\":\n",
    "{\n",
    "\n",
    "\n",
    "\"path\":\n",
    "[\n",
    "\"https://imageprocessor.digital.vistaprint.com/crop/945,117,3210x3210/maxWidth/1000/stockservice.digital.vistaprint.com/9971027207cbc734104f4b327a4118dd.jpg\"\n",
    "],\n",
    "\"label\": [[\"Dog\"]]\n",
    "\n",
    "}\n",
    "}\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "\n",
    "print(request1.get(\"data\", {}))\n",
    "img_path = request1.get(\"data\", {}).get(\"path\")\n",
    "print(str(img_path[0]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'path': ['https://imageprocessor.digital.vistaprint.com/crop/945,117,3210x3210/maxWidth/1000/stockservice.digital.vistaprint.com/9971027207cbc734104f4b327a4118dd.jpg'], 'label': [['Dog']]}\n",
      "https://imageprocessor.digital.vistaprint.com/crop/945,117,3210x3210/maxWidth/1000/stockservice.digital.vistaprint.com/9971027207cbc734104f4b327a4118dd.jpg\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "\n",
    "res = request.urlopen(str(img_path[0])).read()\n",
    "img_array = Image.open(io.BytesIO(res)).resize((224, 224))\n",
    "# img = tf.keras.preprocessing.image.load_img(str(img_path[0]), target_size=(224, 224))\n",
    "img_array = tf.keras.preprocessing.image.img_to_array(img_array)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "img_array = tf.keras.preprocessing.image.img_to_array(img_array)\n",
    "img_array = img_array/255\n",
    "img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
    "\n",
    "# predictions = model1.predict(img_array)\n",
    "predictions = model1(img_array)\n",
    "\n",
    "predictions"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.27777043, 0.72222954]], dtype=float32)>"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "modelsss = tk.models.load_model('CatAndDog.model.h5',custom_objects={'KerasLayer':hub.KerasLayer})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model2 = tf.saved_model.load('/Users/skasmani/Downloads/Redhat/Redhat_git/ml-training/deep-learning/others/mlruns/1/2eddbdfb09af4561946d81d9b9b8c866/artifacts/model/data/model/saved_model.pb')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "# # feature_extractor_layer,\n",
    "# # tf.keras.layers.Dense(num_classes)\n",
    "\n",
    "# # m = tf.keras.Sequential([\n",
    "# #     hub.KerasLayer(\"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b1/feature_vector/2\",\n",
    "# #                    trainable=False),  # Can be True, see below.\n",
    "# #     tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "# # ])\n",
    "# # m.build([None, 240, 240, 3])  # Batch input shape.\n",
    "\n",
    "# with strategy.scope():\n",
    "#     # inputs = tkl.experimental.preprocessing.Rescaling(1./255, input_shape=(IMG_SIZE, IMG_SIZE, 3)),\n",
    "#     inputs = tkl.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "#     # inputs = tkl.experimental.preprocessing.Rescaling(1./255)(inputs)\n",
    "#     x = img_augmentation(inputs)\n",
    "#     x = feature_extractor_layer(x)\n",
    "#     outputs = tkl.Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "\n",
    "#     # outputs = EfficientNetB0(include_top=True, weights=None, classes=NUM_CLASSES)(x)\n",
    "\n",
    "#     model = tk.Model(inputs, outputs)\n",
    "\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "# epochs = 1  # @param {type: \"slider\", min:10, max:100}\n",
    "# hist = model.fit(train_data, epochs=epochs, validation_data=validation_data, verbose=2)"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.save(\"CatAndDog.model.h5\")"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "accuracy = training_history.history[\"accuracy\"]\n",
    "val_accuracy = training_history.history[\"val_accuracy\"]\n",
    "epochs=range(1,epochs)\n",
    "plt.plot(epochs,accuracy, label=\"Training Accuracy\")\n",
    "plt.plot(epochs,val_accuracy, label=\"Validation Accuracy\")\n",
    "\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.legend()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "accuracy = training_history.history[\"loss\"]\n",
    "val_accuracy = training_history.history[\"val_loss\"]\n",
    "epochs=range(1,epochs)\n",
    "plt.plot(epochs,accuracy,  label=\"Training Loss\")\n",
    "plt.plot(epochs,val_accuracy,  label=\"Validation Loss\")\n",
    "\n",
    "\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.legend()\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('AI_ML_38_GPU': conda)"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "interpreter": {
   "hash": "10d220c55dad87bc296f91a276cffc08c658df4f112171a2982baf6251a25e4e"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}